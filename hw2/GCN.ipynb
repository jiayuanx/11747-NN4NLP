{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.read_csv('data/20ng/raw/all_df_mask.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18846 entries, 0 to 18845\n",
      "Data columns (total 7 columns):\n",
      "doc_id        18846 non-null object\n",
      "label         18846 non-null object\n",
      "path          18846 non-null object\n",
      "test_mask     18846 non-null bool\n",
      "text          18846 non-null object\n",
      "train_mask    18846 non-null bool\n",
      "type          18846 non-null object\n",
      "dtypes: bool(2), object(5)\n",
      "memory usage: 920.2+ KB\n"
     ]
    }
   ],
   "source": [
    "all_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>test_mask</th>\n",
       "      <th>text</th>\n",
       "      <th>train_mask</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_id_0</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>data/20ng/20news-bydate-test/alt.atheism/53068</td>\n",
       "      <td>False</td>\n",
       "      <td>decay cbnewsj cb att com \\( dean kaflowitz \\) ...</td>\n",
       "      <td>True</td>\n",
       "      <td>20news-bydate-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_id_1</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>data/20ng/20news-bydate-test/alt.atheism/53257</td>\n",
       "      <td>False</td>\n",
       "      <td>cfaehl vesta unm edu \\( chris faehl \\) subject...</td>\n",
       "      <td>True</td>\n",
       "      <td>20news-bydate-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_id_2</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>data/20ng/20news-bydate-test/alt.atheism/53260</td>\n",
       "      <td>False</td>\n",
       "      <td>mathew mathew mantis co uk subject yet rushdie...</td>\n",
       "      <td>True</td>\n",
       "      <td>20news-bydate-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_id_3</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>data/20ng/20news-bydate-test/alt.atheism/53261</td>\n",
       "      <td>False</td>\n",
       "      <td>dps nasa kodak com \\( dan schaertel , , , \\) s...</td>\n",
       "      <td>True</td>\n",
       "      <td>20news-bydate-test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_id_4</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>data/20ng/20news-bydate-test/alt.atheism/53262</td>\n",
       "      <td>False</td>\n",
       "      <td>halat panther bears \\( jim halat \\) subject 20...</td>\n",
       "      <td>True</td>\n",
       "      <td>20news-bydate-test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     doc_id        label                                            path  \\\n",
       "0  doc_id_0  alt.atheism  data/20ng/20news-bydate-test/alt.atheism/53068   \n",
       "1  doc_id_1  alt.atheism  data/20ng/20news-bydate-test/alt.atheism/53257   \n",
       "2  doc_id_2  alt.atheism  data/20ng/20news-bydate-test/alt.atheism/53260   \n",
       "3  doc_id_3  alt.atheism  data/20ng/20news-bydate-test/alt.atheism/53261   \n",
       "4  doc_id_4  alt.atheism  data/20ng/20news-bydate-test/alt.atheism/53262   \n",
       "\n",
       "   test_mask                                               text  train_mask  \\\n",
       "0      False  decay cbnewsj cb att com \\( dean kaflowitz \\) ...        True   \n",
       "1      False  cfaehl vesta unm edu \\( chris faehl \\) subject...        True   \n",
       "2      False  mathew mathew mantis co uk subject yet rushdie...        True   \n",
       "3      False  dps nasa kodak com \\( dan schaertel , , , \\) s...        True   \n",
       "4      False  halat panther bears \\( jim halat \\) subject 20...        True   \n",
       "\n",
       "                 type  \n",
       "0  20news-bydate-test  \n",
       "1  20news-bydate-test  \n",
       "2  20news-bydate-test  \n",
       "3  20news-bydate-test  \n",
       "4  20news-bydate-test  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix, from_scipy_sparse_matrix\n",
    "from copy import deepcopy\n",
    "import re, math\n",
    "from utils import DocumentStatsBuilder\n",
    "\n",
    "def get_counts(df):\n",
    "    cooccur = defaultdict(int)\n",
    "    doc_freq = defaultdict(int)\n",
    "    for i, row in df.iterrows():\n",
    "        ts = row[\"text\"].split()\n",
    "        wpairs = set([(w1, w2) for w1 in ts for w2 in ts])\n",
    "        ws = set(ts)\n",
    "        for wpair in wpairs:\n",
    "            cooccur[wpair] += 1\n",
    "        for w in ws:\n",
    "            doc_freq[w] += 1\n",
    "    \n",
    "    return cooccur, doc_freq\n",
    "\n",
    "\n",
    "def compute_TFIDF(corpus):\n",
    "#     corpus = df.text.values\n",
    "    counter = CountVectorizer(tokenizer=lambda x: x.split())\n",
    "    tfidf_trans = TfidfTransformer()    \n",
    "    counts = counter.fit_transform(corpus)\n",
    "    \n",
    "    # -- tfidf \n",
    "    print(\"building tfidf...\")\n",
    "    tfidf = tfidf_trans.fit_transform(counts)\n",
    "    # words have specific order when computing the features\n",
    "    # not neccessarily the same to the graph nodes -> needs conversion\n",
    "    features = counter.get_feature_names()\n",
    "    feature2i = dict(zip(features, range(len(features))))\n",
    "    i2feature = dict(zip(range(len(features)), features))\n",
    "    \n",
    "    # -- pmi -- \n",
    "    print(\"building pmi...\")\n",
    "    word_counts = counts.sum(axis=0)\n",
    "    n_words = word_counts.shape[1]\n",
    "    n_docs = tfidf.shape[0]\n",
    "    \n",
    "#     print(\"computing counts\")\n",
    "#     cooccur, doc_freq = get_counts(df)\n",
    "    \n",
    "#     import pdb; pdb.set_trace()\n",
    "#     idx = [k for k in cooccur]\n",
    "#     rows, cols = tuple(zip(*idx))\n",
    "    \n",
    "#     rows = [feature2i[w] for w in rows]\n",
    "#     cols = [feature2i[w] for w in cols]\n",
    "#     print(\"building cooccur_data...\")\n",
    "#     cooccur_data = [math.log(cooccur[(row, col)]*n_docs/(doc_freq[row]*doc_freq[col]))\n",
    "#                     for (row, col) in idx]\n",
    "#     print(\"building cooccur_matrix...\")\n",
    "\n",
    "\n",
    "    pmi = DocumentStatsBuilder.PMI(corpus, feature2i, window_size=2)\n",
    "\n",
    "    idx, cooccur_data = zip(*pmi.items())\n",
    "    rows, cols = tuple(zip(*idx))\n",
    "    rows = [feature2i[w] if w in feature2i else feature2i[\"<unk>\"] for w in rows]\n",
    "    cols = [feature2i[w] if w in feature2i else feature2i[\"<unk>\"] for w in cols]\n",
    "    cooccur_matrix = csr_matrix((cooccur_data, (rows, cols)), shape=(n_words, n_words))\n",
    "    return tfidf, cooccur_matrix, feature2i, i2feature, idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from torch_geometric.utils import to_undirected, is_undirected\n",
    "\n",
    "EMBED_DIM = 300\n",
    "REMOVE_STOP_WORDS = True\n",
    "MIN_COUNTS = 5\n",
    "DEBUG = False\n",
    "\n",
    "class Dataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(Dataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['all_df_mask.tsv']\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['20ng_train.pt']\n",
    "    \n",
    "    def process(self):\n",
    "        df = pd.read_csv(self.raw_paths[0], sep=\"\\t\")\n",
    "        if DEBUG:\n",
    "            df = df.head(1000)\n",
    "        import pdb; pdb.set_trace()\n",
    "        texts = list(map(lambda x: x.split(), df.text.values))\n",
    "        doc_ids = df.doc_id.values\n",
    "        \n",
    "        # convert str labels to ints\n",
    "        labels = df.label.values\n",
    "        unique_labels = df.label.unique()\n",
    "        self.label_dict = dict(zip(unique_labels, range(len(unique_labels))))\n",
    "        labels = [self.label_dict[l] for l in labels]\n",
    "        \n",
    "        doc_label_dict = dict(zip(doc_ids, labels))\n",
    "        is_train_dict = dict(zip(doc_ids, df.train_mask.values))\n",
    "        print(\"building vocab...\")\n",
    "        vocab = Vocab(texts, doc_ids=doc_ids.tolist())\n",
    "        \n",
    "        text_int = vocab.map_dataset_words2index(texts)\n",
    "        \n",
    "        # nodes_idx mapping will be the same as vocab.i2w\n",
    "        print(\"building nodes...\")\n",
    "        n_nodes = len(vocab.w2i)\n",
    "        self.embed = nn.Embedding(n_nodes, EMBED_DIM)\n",
    "        nodes = self.embed(torch.tensor(range(n_nodes)))  # (vocab_size, EMBED_DIM)\n",
    "        \n",
    "        # edges\n",
    "        print(\"building edges...\")\n",
    "        tfidf, cooccur_matrix, edge_feature2i, edge_i2feature, cooccur_idx = compute_TFIDF(df.text.values)\n",
    "        \n",
    "        edge_index = []\n",
    "        edge_attr = []\n",
    "        \n",
    "        # --- doc to words ---\n",
    "        print(\"building doc to word edges...\")\n",
    "        for i, row in df.iterrows():\n",
    "            doc_id = vocab.w2i[row[\"doc_id\"]]\n",
    "            edges_ = zip([doc_id for _ in range(len(text_int[i]))], text_int[i])\n",
    "            edges_ = edges_ \n",
    "            edge_index.extend(list(edges_))\n",
    "        \n",
    "        edge_index = list(set(edge_index))  # a word may occur mult times in a doc\n",
    "        for (doc_id, word_id) in edge_index:\n",
    "            d, w = doc_id, edge_feature2i[vocab.i2w[word_id]]\n",
    "            edge_attr.append(tfidf[d, w])\n",
    "            \n",
    "        # --- words to doc ---\n",
    "        print(\"building word to doc edges...\")\n",
    "        edge_index_back = [(e2, e1) for (e1, e2) in edge_index] \n",
    "        for (word_id, doc_id) in edge_index_back:\n",
    "            d, w = doc_id, edge_feature2i[vocab.i2w[word_id]]\n",
    "            edge_attr.append(tfidf[d, w])\n",
    "        edge_index += edge_index_back\n",
    "            \n",
    "        # --- word to word ---\n",
    "        print(\"building word to word edges...\")\n",
    "        for (w1_edge_idx, w2_edge_idx) in cooccur_idx: \n",
    "            # notice, w1_edge_idx, w2_edge_idx are ints in edge_feature space, \n",
    "            # convert to node feature spaces\n",
    "            w1, w2 = edge_i2feature[w1_edge_idx], edge_i2feature[w2_edge_idx]\n",
    "            w1_vocab_idx, w2_vocab_idx = vocab.w2i[w1], vocab.w2i[w2]\n",
    "\n",
    "            weight = cooccur_matrix[w1_edge_idx, w2_edge_idx] if w1 != w2 else 1\n",
    "            edge_index.append((w1_vocab_idx, w2_vocab_idx))\n",
    "            edge_attr.append(weight)\n",
    "\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "        edge_attr = torch.tensor(edge_attr, dtype=torch.float)\n",
    "        print(\"so many edges...\")\n",
    "        \n",
    "        # --- masks: ---\n",
    "        print(\"building masks...\")\n",
    "        train_mask = []\n",
    "        test_mask = []\n",
    "        for i in range(n_nodes):\n",
    "            w = vocab.i2w[i]\n",
    "            if w in doc_label_dict:\n",
    "                is_train = is_train_dict[w]\n",
    "                train_mask.append(is_train)\n",
    "                test_mask.append(not is_train)\n",
    "            else:\n",
    "                train_mask.append(False)\n",
    "                test_mask.append(False)\n",
    "        labels = np.concatenate((labels, np.array([-1 for i in range(n_nodes-len(labels))])))\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "        print(\"building data...\")\n",
    "        data_list = [Data(x=nodes, y=labels, edge_index=edge_index, edge_attr=edge_attr)]\n",
    "        data_list[0].train_mask = torch.tensor(train_mask)\n",
    "        data_list[0].test_mask = torch.tensor(test_mask)\n",
    "        \n",
    "        data, slices = self.collate(data_list)\n",
    "        print(\"saving...\")\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "class Vocab(object):\n",
    "  \n",
    "  def __init__(self, L, doc_ids=None, remove_stop_words=False, min_counts=1):    \n",
    "    if isinstance(L[0], list):\n",
    "      tokens = list(itertools.chain(*L))\n",
    "      self.token_counts = pd.Series(tokens).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.token_counts.columns = [\"counts\"]\n",
    "      if remove_stop_words:\n",
    "        self.stop_words = set(stopwords.words('english')) \n",
    "        idx = self.token_counts.index.isin(self.stop_words)\n",
    "        self.token_counts = self.token_counts[~idx]\n",
    "      self.token_counts = self.token_counts[self.token_counts.counts >= min_counts]\n",
    "        \n",
    "      self.vocab = [\"<unk>\"] + self.token_counts.index.to_list()\n",
    "    else:\n",
    "      raise Error\n",
    "\n",
    "    self.vocab = list(set(self.vocab))\n",
    "    self.vocab = sorted(self.vocab)\n",
    "    if doc_ids is not None:\n",
    "      self.vocab = doc_ids + self.vocab \n",
    "    \n",
    "    self.w2i = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    self.i2w = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "\n",
    "  def map_words2index(self, L):\n",
    "    return list(map(lambda x: self.w2i[x] if x in self.w2i else self.w2i['<unk>'], L))\n",
    "\n",
    "  def map_index2words(self, L):\n",
    "    return list(map(lambda x: self.i2w[x], L))\n",
    "\n",
    "  def map_words2unk(self, L):\n",
    "    return map(lambda x: x if x in self.vocab else \"<unk>\", L)\n",
    "\n",
    "  def map_dataset_words2index(self, L):\n",
    "    return np.array(list(map(self.map_words2index, L)))\n",
    "\n",
    "  def map_dataset_index2words(self, L):\n",
    "    return np.array(list(map(self.map_index2words, L)))\n",
    "\n",
    "  def map_dataset_words2unk(self, L):\n",
    "    return np.array(list(map(self.map_words2unk, L)))\n",
    "\n",
    "  def remove_sw_helper(self, L):\n",
    "    return filter(lambda x: x not in self.stop_words, L)\n",
    "\n",
    "  def remove_stop_words(self, L):\n",
    "    return np.array(list(map(self.remove_sw_helper, L)))\n",
    "\n",
    "  def get_counts(self):\n",
    "    return self.token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n",
      "61604\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/20ng/raw/all_df_mask.tsv', sep=\"\\t\")\n",
    "texts = list(map(lambda x: x.split(), df.text.values))\n",
    "doc_ids = df.doc_id.values\n",
    "labels = df.label.values\n",
    "doc_label_dict = dict(zip(doc_ids, labels))\n",
    "is_train_dict = dict(zip(doc_ids, df.train_mask.values))\n",
    "print(\"building vocab...\")\n",
    "vocab = Vocab(texts, doc_ids=doc_ids.tolist(),\n",
    "              remove_stop_words=REMOVE_STOP_WORDS, min_counts=MIN_COUNTS)\n",
    "# texts = vocab.remove_stop_words(texts)\n",
    "# texts = vocab.map_dataset_words2unk(texts)\n",
    "# text_int = vocab.map_dataset_words2index(texts)\n",
    "print(len(vocab.w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "> <ipython-input-167-106aafce2481>(29)process()\n",
      "-> texts = list(map(lambda x: x.split(), df.text.values))\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  df.shape\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 7)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846, 7)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "(Pdb)  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building vocab...\n",
      "building nodes...\n",
      "building edges...\n",
      "building tfidf...\n",
      "building pmi...\n",
      "building doc to word edges...\n",
      "building word to doc edges...\n",
      "building word to word edges...\n",
      "so many edges...\n",
      "building masks...\n",
      "building data...\n",
      "saving...\n",
      "Done!\n",
      "CPU times: user 3min 35s, sys: 3.32 s, total: 3min 38s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%time dataset = Dataset('data/20ng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_attr=[6980982], edge_index=[2, 6980982], test_mask=[61604], train_mask=[61604], x=[61604, 300], y=[61604])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Net\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(dataset.num_node_features, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, dataset.num_classes)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_DIM = 200\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(HIDDEN_DIM).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005) #, weight_decay=5e-4)\n",
    "model.train()\n",
    "start = time.time()\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 9:\n",
    "        _, pred = model(data).max(dim=1)\n",
    "        train_correct = float(pred[data.train_mask].eq(data.y[data.train_mask]).sum().item())\n",
    "        train_acc = train_correct / data.train_mask.sum().item()\n",
    "        valid_correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "        valid_acc = valid_correct / data.test_mask.sum().item()\n",
    "        \n",
    "        print(\"epoch {}, loss = {:.4f}, train_acc = {:.4f}, valid_acc = {:.4f}, time taken: {:.2f}\".format(epoch, loss, train_acc, valid_acc, time.time()-start))\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "_, pred = model(data).max(dim=1)\n",
    "correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "acc = correct / data.test_mask.sum().item()\n",
    "print('Accuracy: {:.4f}'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
