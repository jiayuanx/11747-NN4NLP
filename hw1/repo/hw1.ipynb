{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-A2teSE33Ip"
   },
   "source": [
    "### HW1 Text Classifier\n",
    "---\n",
    "\n",
    "#### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "uR0QbHxyxG8u",
    "outputId": "a6456e1c-e805-4e11-8b58-e4f92f2e5d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-03 22:37:53--  http://phontron.com/data/topicclass-v1.tar.gz\n",
      "Resolving phontron.com (phontron.com)... 208.113.196.149\n",
      "Connecting to phontron.com (phontron.com)|208.113.196.149|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15665160 (15M) [application/gzip]\n",
      "Saving to: ‘topicclass-v1.tar.gz’\n",
      "\n",
      "topicclass-v1.tar.g 100%[===================>]  14.94M  33.2MB/s    in 0.4s    \n",
      "\n",
      "2020-02-03 22:37:53 (33.2 MB/s) - ‘topicclass-v1.tar.gz’ saved [15665160/15665160]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://phontron.com/data/topicclass-v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "KWh76FTtxLer",
    "outputId": "d706d38c-e14f-4d05-be4a-a314fffacf0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topicclass/\n",
      "topicclass/topicclass_valid.txt\n",
      "topicclass/topicclass_test.txt\n",
      "topicclass/topicclass_train.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf topicclass-v1.tar.gz topicclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UoIBRfq30dM"
   },
   "source": [
    "#### Data Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wFETkOfbx7Op"
   },
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "  with open(path, \"r\") as f:\n",
    "    data = f.readlines()\n",
    "  labels, text = zip(*map(lambda x: x.split(\"|||\"), data))\n",
    "  \n",
    "  labels = map(lambda x: x.strip(\"\\n\").strip().lower(), labels)\n",
    "  labels = list(map(lambda x: \"media and drama\" if \"media and darama\" in x else x, labels))\n",
    "  text = list(map(lambda x: x.strip(\"\\n\").strip().lower(), text))\n",
    "  return text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OXjBqyex7Rl"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = read_data(\"topicclass/topicclass_train.txt\")\n",
    "valid_x, valid_y = read_data(\"topicclass/topicclass_valid.txt\")\n",
    "test_x, test_y = read_data(\"topicclass/topicclass_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaVfanQryWPU"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "word_tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def clean_string(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\-\\_]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\\\\", \" \", string)\n",
    "    string = re.sub(r\"\\s+\", \" \", string)\n",
    "    return string.strip()\n",
    "\n",
    "def tokenize(string):\n",
    "    # return list(map(lemmatizer.lemmatize, word_tokenizer.tokenize(string)))\n",
    "    return  word_tokenizer.tokenize(string)\n",
    "\n",
    "def preprocess(texts):\n",
    "    texts = map(clean_string, texts)\n",
    "    texts = map(tokenize, texts)\n",
    "    return list(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "bNNlB6B0yWUV",
    "outputId": "7e80983c-3fa8-4796-f862-2eb5f96a9097"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.4 s, sys: 145 ms, total: 27.5 s\n",
      "Wall time: 27.5 s\n",
      "CPU times: user 67.3 ms, sys: 0 ns, total: 67.3 ms\n",
      "Wall time: 67.2 ms\n",
      "CPU times: user 72.1 ms, sys: 0 ns, total: 72.1 ms\n",
      "Wall time: 72 ms\n"
     ]
    }
   ],
   "source": [
    "%time train_x = preprocess(train_x)\n",
    "%time valid_x = preprocess(valid_x)\n",
    "%time test_x = preprocess(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eDZnzu201NRq",
    "outputId": "51f44fb1-87a6-4a95-cca7-9be2d0de348c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "len_train = list(map(len, train_x))\n",
    "pd.Series(len_train).quantile(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XnI6mBbYyWSl"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class Vocab(object):\n",
    "  \n",
    "  def __init__(self, L):\n",
    "    if isinstance(L[0], list):\n",
    "      tokens = list(itertools.chain(*L))\n",
    "      self.token_counts = pd.Series(tokens).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = [\"unk\"] + self.token_counts.index.to_list()\n",
    "    else:\n",
    "      tokens = self.token_counts = pd.Series(L).value_counts().to_frame().sort_index(ascending=True)\n",
    "      self.vocab = self.token_counts.index.to_list()\n",
    "    self.w2i = dict(zip(self.vocab, range(len(self.vocab))))\n",
    "    self.i2w = dict(zip(range(len(self.vocab)), self.vocab))\n",
    "\n",
    "  def map_words2index(self, L):\n",
    "    return list(map(lambda x: self.w2i[x] if x in self.w2i else self.w2i['unk'], L))\n",
    "\n",
    "  def map_index2words(self, L):\n",
    "    return list(map(lambda x: self.i2w[x], L))\n",
    "\n",
    "  def map_dataset_words2index(self, L):\n",
    "    return np.array(list(map(self.map_words2index, L)))\n",
    "\n",
    "  def map_dataset_index2words(self, L):\n",
    "    return np.array(list(map(self.map_index2words, L)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "Bt8Ig3B91pcL",
    "outputId": "f717e53a-23df-4a48-9809-4422b7f68bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.17 s, sys: 20.5 ms, total: 1.19 s\n",
      "Wall time: 1.2 s\n",
      "CPU times: user 2.06 s, sys: 16.4 ms, total: 2.08 s\n",
      "Wall time: 2.08 s\n",
      "CPU times: user 4.56 ms, sys: 28 µs, total: 4.59 ms\n",
      "Wall time: 4.55 ms\n",
      "CPU times: user 5.45 ms, sys: 0 ns, total: 5.45 ms\n",
      "Wall time: 5.39 ms\n"
     ]
    }
   ],
   "source": [
    "%time vocab = Vocab(train_x + valid_x)\n",
    "%time train_x = vocab.map_dataset_words2index(train_x)\n",
    "%time valid_x = vocab.map_dataset_words2index(valid_x)\n",
    "%time test_x = vocab.map_dataset_words2index(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "AuJJA1Fn1pi4",
    "outputId": "9eba96bb-f689-497a-d4e2-29afd4bbbf3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'several of these rights regulate pre - trial procedure access to a non - excessive bail , the right to indictment by a grand jury , the right to an information ( charging document ) , the right to a speedy trial , and the right to be tried in a specific venue'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(vocab.vocab))\n",
    "train_x_ = vocab.map_dataset_index2words(train_x)\n",
    "\" \".join(train_x_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "nFwreiw01pfA",
    "outputId": "91ffd6f7-48e7-4bbe-b6e8-bd0b7aacad6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agriculture, food and drink': 0,\n",
       " 'art and architecture': 1,\n",
       " 'engineering and technology': 2,\n",
       " 'geography and places': 3,\n",
       " 'history': 4,\n",
       " 'language and literature': 5,\n",
       " 'mathematics': 6,\n",
       " 'media and drama': 7,\n",
       " 'miscellaneous': 8,\n",
       " 'music': 9,\n",
       " 'natural sciences': 10,\n",
       " 'philosophy and religion': 11,\n",
       " 'social sciences and society': 12,\n",
       " 'sports and recreation': 13,\n",
       " 'video games': 14,\n",
       " 'warfare': 15}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vocab = Vocab(train_y + valid_y)\n",
    "label_vocab.w2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1bpNf7_84BN"
   },
   "outputs": [],
   "source": [
    "train_y = label_vocab.map_words2index(train_y)\n",
    "valid_y = label_vocab.map_words2index(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PcCgk4-C-vu6",
    "outputId": "726d8bc7-02aa-4fd6-f4e1-fd36c1054097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 13, 13, 7, 9, 9, 9, 7, 12, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.utils import to_categorical\n",
    "\n",
    "# train_y = to_categorical(train_y, num_classes=17)\n",
    "# train_y = list(map(list, train_y))\n",
    "# valid_y = to_categorical(valid_y, num_classes=17)\n",
    "# valid_y = list(map(list, valid_y))\n",
    "valid_y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcRBl0KyStAU"
   },
   "source": [
    "#### Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ytengrB2StVg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, LongTensor\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "from IPython.core.debugger import set_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zmlh9vnMStc_"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self, X, Y=None):\n",
    "    self.X = X\n",
    "    self.Y = Y\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    if self.Y is not None:\n",
    "      return (self.X[idx], self.Y[idx])\n",
    "    return (self.X[idx], None)\n",
    "\n",
    "def pad(seq, seq_lengths, pad_after=True):\n",
    "  max_seq_len = max(seq_lengths)\n",
    "  seq_tensor = Variable(torch.zeros((len(seq), max_seq_len))).long()\n",
    "  # pad input tensor\n",
    "  for idx, seq in enumerate(seq):\n",
    "    seq_len = seq_lengths[idx]\n",
    "    if pad_after:\n",
    "      seq_tensor[idx, :seq_len] = LongTensor(np.asarray(seq).astype(int))\n",
    "    else: \n",
    "      # pad before\n",
    "      seq_tensor[idx, max_seq_len-seq_len:] = LongTensor(np.asarray(seq).astype(int))\n",
    "  return seq_tensor\n",
    "\n",
    "def batchify(data):\n",
    "  X, Y = tuple(map(list, zip(*data)))\n",
    "  seq_lengths = LongTensor([len(x) for x in X])\n",
    "  X = pad(X, seq_lengths, pad_after=True)\n",
    "  Y = LongTensor(Y)\n",
    "  return X, Y\n",
    "\n",
    "def batchify_test(data):\n",
    "  X, Y = tuple(map(list, zip(*data)))\n",
    "  seq_lengths = LongTensor([len(x) for x in X])\n",
    "  X = pad(X, seq_lengths, pad_after=True)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "train = MyDataset(train_x, train_y)\n",
    "valid = MyDataset(valid_x, valid_y)\n",
    "test = MyDataset(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BWkiAoWStks"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=64, shuffle=True, collate_fn=batchify)\n",
    "valid_loader = DataLoader(valid, batch_size=64, shuffle=False, collate_fn=batchify)\n",
    "test_loader = DataLoader(test, batch_size=64, shuffle=False, collate_fn=batchify_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZE7E6FYw3vZK"
   },
   "source": [
    "#### Model Fitting\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t-4Q8oVzxTsl"
   },
   "outputs": [],
   "source": [
    "from torch import nn, LongTensor, Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIY40pvY9fgR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def accuracy(preds, y):\n",
    "  return (np.array(preds) == np.array(y)).astype(int).mean()\n",
    "\n",
    "\n",
    "def train_epoch(epoch, model, optimizer, criterion):\n",
    "  model.train()\n",
    "  train_loss, n_data = 0, 0\n",
    "  start = time.time()\n",
    "  preds = []\n",
    "  labels = []\n",
    "  for i, (x, y) in enumerate(train_loader):\n",
    "    n_data += x.size()[0]\n",
    "    labels.extend(y.tolist())\n",
    "    if is_cuda: x, y = x.cuda(), y.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "    loss = criterion(out, y)\n",
    "    loss.backward()\n",
    "    if grad_clip: torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    optimizer.step()\n",
    "    train_loss += loss\n",
    "    if i % print_iter == print_iter - 1:\n",
    "      model, valid_preds, valid_labels, valid_loss = validate(model, criterion)\n",
    "      print(\"\"\"epoch {} - batch [{}/{}] - train loss: {:.2f} - acc: {:.2f} - valid loss : {:.2f} - acc : {:.2f} time taken: {:.2f}\"\"\".format(epoch, i, \n",
    "            len(train_loader), train_loss/(i+1),\n",
    "            accuracy(preds, labels), valid_loss, accuracy(valid_preds, valid_labels),\n",
    "            time.time()-start), flush=True)\n",
    "      \n",
    "      model.train()\n",
    "      start = time.time()\n",
    "      train_loss = 0\n",
    "\n",
    "  # end of epoch\n",
    "  model, valid_preds, valid_labels, valid_loss = validate(model, criterion)\n",
    "  print(\"\"\"epoch {} - batch [{}/{}] - train loss: {:.2f} - acc: {:.2f} - valid loss : {:.2f} - acc : {:.2f} time taken: {:.2f}\"\"\".format(epoch, i, \n",
    "        len(train_loader), train_loss/(i+1),\n",
    "        accuracy(preds, labels), valid_loss, accuracy(valid_preds, valid_labels),\n",
    "        time.time()-start), flush=True)\n",
    "  return model\n",
    "\n",
    "def learning_rate_decay(optimizer):\n",
    "  for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * 0.3\n",
    "  return optimizer\n",
    "\n",
    "def training(model, epoches):\n",
    "  if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  for ep in range(epochs):\n",
    "    model = train_epoch(ep, model, optimizer, criterion)\n",
    "    optimizer = learning_rate_decay(optimizer)\n",
    "  return model\n",
    "\n",
    "def validate(model, criterion):\n",
    "  model.eval()\n",
    "  valid_loss = 0\n",
    "  preds, labels = [], []\n",
    "  for i, (x, y) in enumerate(valid_loader):\n",
    "    labels.extend(y.tolist())\n",
    "    if torch.cuda.is_available(): x, y = x.cuda(), y.cuda()\n",
    "    out = model(x)\n",
    "    loss = criterion(out, y)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "    valid_loss += loss\n",
    "  return model, preds, labels, valid_loss/(i+1)\n",
    "    \n",
    "def predict(model, loader):\n",
    "  model.eval()\n",
    "  preds, labels = [], []\n",
    "  for i, (x, _) in enumerate(loader):\n",
    "    if torch.cuda.is_available(): x = x.cuda()\n",
    "    out = model(x)\n",
    "    preds.extend(out.argmax(axis=1).tolist())\n",
    "  return preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Zv3GeBOXZ_y"
   },
   "source": [
    "#### Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnH8gLLVXZNQ"
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class LSTM_clf(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               layers=1, bidirectional=False):\n",
    "    super(LSTM_clf, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.net = nn.LSTM(embed_dim, hidden_dim,  num_layers=layers, \n",
    "                       bidirectional=bidirectional, dropout=0.5)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.bn = nn.BatchNorm1d(hidden_dim * (int(bidirectional) + 1))\n",
    "    self.linear = nn.Linear(hidden_dim * (int(bidirectional) + 1), out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x)\n",
    "    out = self.net(out)[0]\n",
    "    out = self.relu(out).transpose(1,2)\n",
    "    out = F.max_pool1d(out, out.size()[2]).squeeze()\n",
    "    out = self.linear(self.bn(out))\n",
    "    return out\n",
    "\n",
    "class DCNN_block(nn.Module):\n",
    "  \n",
    "  def __init__(self, embed_dim, hidden_dim, kernel_size, dilations=None,\n",
    "               dropout=0.2):\n",
    "    super(DCNN_block, self).__init__()\n",
    "    self.conv1 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=1))\n",
    "    self.conv2 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=2))\n",
    "    self.conv3 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, dilation=4))\n",
    "    self.net = nn.Sequential(self.conv1, nn.ReLU(), nn.Dropout(dropout),\n",
    "                             self.conv2, nn.ReLU(), nn.Dropout(dropout), \n",
    "                             self.conv3, nn.ReLU(), nn.Dropout(dropout))\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # N x C x L\n",
    "    return self.net(x)\n",
    "\n",
    "class DCNN_rez_block(nn.Module):\n",
    "  \n",
    "  def __init__(self, embed_dim, hidden_dim, kernel_size, dilations=None,\n",
    "               dropout=0.2):\n",
    "    super(DCNN_rez_block, self).__init__()\n",
    "    self.conv1 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*1, dilation=1))\n",
    "    self.conv2 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*2, dilation=2))\n",
    "    self.conv3 = weight_norm(nn.Conv1d(embed_dim, hidden_dim, kernel_size, \n",
    "                                       padding=(kernel_size-1)*4, dilation=4))\n",
    "\n",
    "    self.relu1 = nn.ReLU()\n",
    "    self.relu2 = nn.ReLU()\n",
    "    self.relu3 = nn.ReLU()\n",
    "\n",
    "    self.do1 = nn.Dropout(dropout)\n",
    "    self.do2 = nn.Dropout(dropout)\n",
    "    self.do3 = nn.Dropout(dropout)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # N x C x L\n",
    "    seq_len = x.size()[2]\n",
    "    out = self.do1(self.relu1(self.conv1(x)))[:, :, -seq_len:]\n",
    "    out = out + self.do2(self.relu2(self.conv2(x)))[:, :, -seq_len:]\n",
    "    out = out + self.do3(self.relu3(self.conv3(x)))[:, :, -seq_len:]\n",
    "    return out\n",
    "\n",
    "\n",
    "class DCNN(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               kernel_size, dilations=None, rez_block=True, \n",
    "               dropout=0.2):\n",
    "    super(DCNN, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    if rez_block: \n",
    "      self.net = DCNN_rez_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    else:\n",
    "      self.net = DCNN_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    self.bn = nn.BatchNorm1d(hidden_dim)\n",
    "    self.linear = nn.Linear(hidden_dim, out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x)\n",
    "    out = self.net(out.transpose(1,2))\n",
    "    out = F.max_pool1d(out, out.size()[2]).squeeze()\n",
    "    out = self.linear(self.bn(out))\n",
    "    return out\n",
    "\n",
    "\n",
    "class DDCNN(nn.Module):\n",
    "  # Dilated and Dense CNN\n",
    "  def __init__(self, embed_dim, hidden_dim, vocab_size, out_size, \n",
    "               kernel_size, dilations=None, rez_block=True, \n",
    "               dropout=0.2):\n",
    "    super(DDCNN, self).__init__()\n",
    "    self.word_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    if rez_block: \n",
    "      self.dcnn = DCNN_rez_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "    else:\n",
    "      self.dcnn = DCNN_block(embed_dim, hidden_dim, kernel_size, dilations, dropout)\n",
    "\n",
    "    self.cnn1 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 4, padding=3, dilation=1))\n",
    "    self.cnn2 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 6, padding=5, dilation=1))\n",
    "    self.cnn3 = weight_norm(nn.Conv1d(embed_dim, int(hidden_dim//3), 8, padding=7, dilation=1))\n",
    "    \n",
    "    self.bn = nn.BatchNorm1d(hidden_dim*2)\n",
    "    self.linear = nn.Linear(hidden_dim*2, out_size)\n",
    "\n",
    "  def cnn(self, x):\n",
    "    out1 = F.relu(self.cnn1(x))\n",
    "    out2 = F.relu(self.cnn2(x))\n",
    "    out3 = F.relu(self.cnn3(x))\n",
    "    outs = []\n",
    "    for o in [out1, out2, out3]:\n",
    "      outs.append(F.max_pool1d(o, o.size()[2]).squeeze())\n",
    "    out = torch.cat(outs, 1)\n",
    "    return out\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = self.word_embedding(x).transpose(1,2)\n",
    "    dcnn_out = self.dcnn(out)\n",
    "\n",
    "    cnn_out = self.cnn(out)\n",
    "    dcnn_out = F.max_pool1d(dcnn_out, dcnn_out.size()[2]).squeeze()\n",
    "    out = self.linear(self.bn(torch.cat((dcnn_out,cnn_out), 1)))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "frS_s9lqxVlh",
    "outputId": "55633d32-02ee-48b6-906b-39e50eaf20d0"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "bs = 64\n",
    "n_class = 16\n",
    "epochs = 3\n",
    "lstm_hidden = 300\n",
    "cnn_hidden = 300\n",
    "embed_dim = 300\n",
    "layers = 2\n",
    "kernel_size = 3\n",
    "vocab_size = len(vocab.vocab)\n",
    "is_cuda = torch.cuda.is_available()\n",
    "lr = 0.001\n",
    "grad_clip = 1\n",
    "print_iter = 500\n",
    "lstm1 = LSTM_clf(embed_dim, lstm_hidden, vocab_size, n_class, layers)\n",
    "dcnn1 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, kernel_size, \n",
    "             rez_block=False, dropout=0.5)\n",
    "dcnn_rez1 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, kernel_size, \n",
    "                 rez_block=True, dropout=0.5)\n",
    "dcnn2 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, kernel_size, \n",
    "             rez_block=False, dropout=0.2)\n",
    "dcnn_rez2 = DCNN(embed_dim, cnn_hidden, vocab_size, n_class, kernel_size, \n",
    "                 rez_block=True, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddcnn_rez2 = DDCNN(embed_dim, 150, vocab_size, n_class, kernel_size, \n",
    "                 rez_block=True, dropout=0.2)\n",
    "ddcnn_rez1 = DDCNN(embed_dim, 300, vocab_size, n_class, kernel_size, \n",
    "                 rez_block=True, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcYULph68b_g"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=bs, shuffle=True, collate_fn=batchify)\n",
    "valid_loader = DataLoader(valid, batch_size=bs, shuffle=False, collate_fn=batchify)\n",
    "test_loader = DataLoader(test, batch_size=bs, shuffle=False, collate_fn=batchify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "oaJ5GfhRh-4w",
    "outputId": "2ac4011c-5e7a-47ce-bd63-c4a601a16621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 1.88 - acc: 0.43 - valid loss : 1.53 - acc : 0.53 time taken: 8.81\n",
      "epoch 0 - batch [999/3968] - train loss: 0.73 - acc: 0.49 - valid loss : 1.34 - acc : 0.62 time taken: 8.79\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.44 - acc: 0.53 - valid loss : 1.12 - acc : 0.68 time taken: 8.80\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.30 - acc: 0.55 - valid loss : 1.11 - acc : 0.67 time taken: 8.84\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.23 - acc: 0.57 - valid loss : 1.05 - acc : 0.72 time taken: 8.85\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.18 - acc: 0.59 - valid loss : 1.03 - acc : 0.74 time taken: 8.86\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.15 - acc: 0.60 - valid loss : 0.94 - acc : 0.74 time taken: 8.91\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.12 - acc: 0.62 - valid loss : 1.02 - acc : 0.74 time taken: 8.50\n",
      "epoch 1 - batch [499/3968] - train loss: 0.85 - acc: 0.74 - valid loss : 0.95 - acc : 0.76 time taken: 8.87\n",
      "epoch 1 - batch [999/3968] - train loss: 0.42 - acc: 0.74 - valid loss : 0.93 - acc : 0.77 time taken: 8.91\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.28 - acc: 0.74 - valid loss : 0.92 - acc : 0.77 time taken: 8.89\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.21 - acc: 0.74 - valid loss : 0.87 - acc : 0.79 time taken: 8.88\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.16 - acc: 0.74 - valid loss : 0.86 - acc : 0.78 time taken: 8.88\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.13 - acc: 0.74 - valid loss : 0.89 - acc : 0.79 time taken: 8.89\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.11 - acc: 0.75 - valid loss : 0.90 - acc : 0.79 time taken: 8.86\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.09 - acc: 0.75 - valid loss : 0.85 - acc : 0.79 time taken: 8.35\n",
      "epoch 2 - batch [499/3968] - train loss: 0.72 - acc: 0.78 - valid loss : 0.86 - acc : 0.80 time taken: 8.87\n",
      "epoch 2 - batch [999/3968] - train loss: 0.36 - acc: 0.78 - valid loss : 0.88 - acc : 0.80 time taken: 8.89\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.24 - acc: 0.78 - valid loss : 0.86 - acc : 0.80 time taken: 9.07\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.18 - acc: 0.78 - valid loss : 0.85 - acc : 0.80 time taken: 9.06\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.14 - acc: 0.78 - valid loss : 0.86 - acc : 0.80 time taken: 9.13\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.12 - acc: 0.78 - valid loss : 0.85 - acc : 0.81 time taken: 9.74\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.10 - acc: 0.78 - valid loss : 0.85 - acc : 0.80 time taken: 9.05\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.09 - acc: 0.78 - valid loss : 0.86 - acc : 0.80 time taken: 8.35\n",
      "CPU times: user 3min 4s, sys: 28 s, total: 3min 32s\n",
      "Wall time: 3min 33s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (net): DCNN_rez_block(\n",
       "    (conv1): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (relu1): ReLU()\n",
       "    (relu2): ReLU()\n",
       "    (relu3): ReLU()\n",
       "    (do1): Dropout(p=0.5, inplace=False)\n",
       "    (do2): Dropout(p=0.5, inplace=False)\n",
       "    (do3): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(dcnn_rez1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "colab_type": "code",
    "id": "udcd1KyjnwdZ",
    "outputId": "edc0bea2-f464-4db2-f6c0-ba64f63f1ff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 1.78 - acc: 0.45 - valid loss : 1.22 - acc : 0.63 time taken: 8.86\n",
      "epoch 0 - batch [999/3968] - train loss: 0.69 - acc: 0.52 - valid loss : 1.14 - acc : 0.67 time taken: 8.90\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.41 - acc: 0.55 - valid loss : 1.00 - acc : 0.71 time taken: 8.88\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.29 - acc: 0.58 - valid loss : 0.97 - acc : 0.72 time taken: 8.88\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.22 - acc: 0.59 - valid loss : 1.04 - acc : 0.72 time taken: 8.91\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.17 - acc: 0.61 - valid loss : 0.87 - acc : 0.74 time taken: 8.92\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.14 - acc: 0.62 - valid loss : 0.91 - acc : 0.75 time taken: 8.95\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.11 - acc: 0.63 - valid loss : 0.89 - acc : 0.75 time taken: 8.35\n",
      "epoch 1 - batch [499/3968] - train loss: 0.78 - acc: 0.76 - valid loss : 0.84 - acc : 0.77 time taken: 8.89\n",
      "epoch 1 - batch [999/3968] - train loss: 0.38 - acc: 0.76 - valid loss : 0.80 - acc : 0.77 time taken: 9.59\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.25 - acc: 0.77 - valid loss : 0.80 - acc : 0.77 time taken: 9.64\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.18 - acc: 0.77 - valid loss : 0.79 - acc : 0.78 time taken: 9.33\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.15 - acc: 0.77 - valid loss : 0.79 - acc : 0.78 time taken: 9.30\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.12 - acc: 0.77 - valid loss : 0.79 - acc : 0.77 time taken: 9.55\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.10 - acc: 0.77 - valid loss : 0.77 - acc : 0.78 time taken: 8.99\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.08 - acc: 0.77 - valid loss : 0.76 - acc : 0.79 time taken: 8.40\n",
      "epoch 2 - batch [499/3968] - train loss: 0.61 - acc: 0.81 - valid loss : 0.74 - acc : 0.79 time taken: 10.04\n",
      "epoch 2 - batch [999/3968] - train loss: 0.30 - acc: 0.81 - valid loss : 0.74 - acc : 0.79 time taken: 8.94\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.20 - acc: 0.81 - valid loss : 0.74 - acc : 0.80 time taken: 8.91\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.15 - acc: 0.81 - valid loss : 0.75 - acc : 0.79 time taken: 8.94\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.12 - acc: 0.81 - valid loss : 0.75 - acc : 0.79 time taken: 8.94\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.10 - acc: 0.81 - valid loss : 0.74 - acc : 0.79 time taken: 8.91\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.09 - acc: 0.81 - valid loss : 0.75 - acc : 0.79 time taken: 8.93\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.07 - acc: 0.81 - valid loss : 0.75 - acc : 0.79 time taken: 8.36\n",
      "CPU times: user 3min 2s, sys: 32.8 s, total: 3min 35s\n",
      "Wall time: 3min 36s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (net): DCNN_rez_block(\n",
       "    (conv1): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (relu1): ReLU()\n",
       "    (relu2): ReLU()\n",
       "    (relu3): ReLU()\n",
       "    (do1): Dropout(p=0.2, inplace=False)\n",
       "    (do2): Dropout(p=0.2, inplace=False)\n",
       "    (do3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(dcnn_rez2, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 806
    },
    "colab_type": "code",
    "id": "zqhNp-fMZyeV",
    "outputId": "cef6a10f-bcec-48d0-efb2-e165832e3287"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 2.41 - acc: 0.21 - valid loss : 3.59 - acc : 0.08 time taken: 7.95\n",
      "epoch 0 - batch [999/3968] - train loss: 0.81 - acc: 0.35 - valid loss : 1.37 - acc : 0.63 time taken: 7.99\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.45 - acc: 0.43 - valid loss : 1.30 - acc : 0.59 time taken: 7.99\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.31 - acc: 0.48 - valid loss : 1.25 - acc : 0.66 time taken: 8.00\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.23 - acc: 0.51 - valid loss : 1.17 - acc : 0.68 time taken: 8.00\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.18 - acc: 0.54 - valid loss : 1.19 - acc : 0.68 time taken: 8.00\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.15 - acc: 0.56 - valid loss : 1.08 - acc : 0.70 time taken: 8.00\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.12 - acc: 0.57 - valid loss : 1.05 - acc : 0.73 time taken: 7.47\n",
      "epoch 1 - batch [499/3968] - train loss: 0.91 - acc: 0.72 - valid loss : 1.04 - acc : 0.74 time taken: 7.94\n",
      "epoch 1 - batch [999/3968] - train loss: 0.44 - acc: 0.73 - valid loss : 1.02 - acc : 0.75 time taken: 7.93\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.29 - acc: 0.73 - valid loss : 1.02 - acc : 0.75 time taken: 7.94\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.21 - acc: 0.73 - valid loss : 1.01 - acc : 0.76 time taken: 7.95\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.17 - acc: 0.73 - valid loss : 1.01 - acc : 0.76 time taken: 7.91\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.14 - acc: 0.73 - valid loss : 1.01 - acc : 0.76 time taken: 7.93\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.12 - acc: 0.73 - valid loss : 0.95 - acc : 0.77 time taken: 7.92\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.10 - acc: 0.74 - valid loss : 1.00 - acc : 0.76 time taken: 7.41\n",
      "epoch 2 - batch [499/3968] - train loss: 0.78 - acc: 0.76 - valid loss : 0.99 - acc : 0.77 time taken: 7.90\n",
      "epoch 2 - batch [999/3968] - train loss: 0.39 - acc: 0.76 - valid loss : 0.99 - acc : 0.77 time taken: 7.91\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.26 - acc: 0.76 - valid loss : 0.98 - acc : 0.77 time taken: 7.91\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.19 - acc: 0.76 - valid loss : 0.96 - acc : 0.77 time taken: 7.91\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.15 - acc: 0.76 - valid loss : 0.97 - acc : 0.77 time taken: 7.91\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.13 - acc: 0.76 - valid loss : 0.98 - acc : 0.78 time taken: 7.92\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.11 - acc: 0.76 - valid loss : 0.97 - acc : 0.77 time taken: 7.93\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.09 - acc: 0.76 - valid loss : 0.96 - acc : 0.78 time taken: 7.43\n",
      "CPU times: user 2min 43s, sys: 25 s, total: 3min 8s\n",
      "Wall time: 3min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (net): DCNN_block(\n",
       "    (conv1): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
       "    (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "      (6): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(dcnn1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 2.07 - acc: 0.35 - valid loss : 1.47 - acc : 0.55 time taken: 7.97\n",
      "epoch 0 - batch [999/3968] - train loss: 0.71 - acc: 0.45 - valid loss : 1.18 - acc : 0.65 time taken: 8.01\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.40 - acc: 0.51 - valid loss : 1.08 - acc : 0.69 time taken: 7.99\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.28 - acc: 0.55 - valid loss : 0.96 - acc : 0.72 time taken: 7.99\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.21 - acc: 0.57 - valid loss : 0.97 - acc : 0.74 time taken: 7.99\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.17 - acc: 0.59 - valid loss : 0.92 - acc : 0.75 time taken: 8.00\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.14 - acc: 0.61 - valid loss : 0.89 - acc : 0.76 time taken: 8.01\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.11 - acc: 0.62 - valid loss : 0.88 - acc : 0.77 time taken: 7.48\n",
      "epoch 1 - batch [499/3968] - train loss: 0.78 - acc: 0.76 - valid loss : 0.84 - acc : 0.78 time taken: 7.90\n",
      "epoch 1 - batch [999/3968] - train loss: 0.38 - acc: 0.76 - valid loss : 0.83 - acc : 0.78 time taken: 7.91\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.25 - acc: 0.76 - valid loss : 0.84 - acc : 0.79 time taken: 7.93\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.19 - acc: 0.77 - valid loss : 0.81 - acc : 0.80 time taken: 7.95\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.15 - acc: 0.77 - valid loss : 0.80 - acc : 0.79 time taken: 7.92\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.12 - acc: 0.77 - valid loss : 0.83 - acc : 0.79 time taken: 7.93\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.11 - acc: 0.77 - valid loss : 0.82 - acc : 0.78 time taken: 7.94\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.09 - acc: 0.77 - valid loss : 0.81 - acc : 0.79 time taken: 7.42\n",
      "epoch 2 - batch [499/3968] - train loss: 0.65 - acc: 0.80 - valid loss : 0.79 - acc : 0.79 time taken: 7.91\n",
      "epoch 2 - batch [999/3968] - train loss: 0.33 - acc: 0.80 - valid loss : 0.79 - acc : 0.80 time taken: 7.92\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.22 - acc: 0.80 - valid loss : 0.81 - acc : 0.80 time taken: 7.92\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.16 - acc: 0.80 - valid loss : 0.79 - acc : 0.80 time taken: 7.92\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.13 - acc: 0.80 - valid loss : 0.78 - acc : 0.80 time taken: 7.95\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.11 - acc: 0.80 - valid loss : 0.81 - acc : 0.80 time taken: 7.95\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.09 - acc: 0.80 - valid loss : 0.80 - acc : 0.80 time taken: 7.95\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.08 - acc: 0.80 - valid loss : 0.82 - acc : 0.80 time taken: 7.45\n",
      "CPU times: user 2min 40s, sys: 27.9 s, total: 3min 8s\n",
      "Wall time: 3min 9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (net): DCNN_block(\n",
       "    (conv1): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
       "    (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "    (net): Sequential(\n",
       "      (0): Conv1d(300, 300, kernel_size=(3,), stride=(1,))\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(2,))\n",
       "      (4): ReLU()\n",
       "      (5): Dropout(p=0.2, inplace=False)\n",
       "      (6): Conv1d(300, 300, kernel_size=(3,), stride=(1,), dilation=(4,))\n",
       "      (7): ReLU()\n",
       "      (8): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(dcnn2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "colab_type": "code",
    "id": "ditzSZC9fMM2",
    "outputId": "ba333222-c58e-4234-f58e-8a912d76cc77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 2.18 - acc: 0.31 - valid loss : 1.48 - acc : 0.54 time taken: 12.11\n",
      "epoch 0 - batch [999/3968] - train loss: 0.76 - acc: 0.42 - valid loss : 1.41 - acc : 0.63 time taken: 12.19\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.44 - acc: 0.48 - valid loss : 1.28 - acc : 0.67 time taken: 12.19\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.30 - acc: 0.52 - valid loss : 1.15 - acc : 0.72 time taken: 12.20\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.23 - acc: 0.55 - valid loss : 1.14 - acc : 0.72 time taken: 12.21\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.18 - acc: 0.57 - valid loss : 1.12 - acc : 0.72 time taken: 12.26\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.15 - acc: 0.58 - valid loss : 1.06 - acc : 0.73 time taken: 12.26\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.12 - acc: 0.59 - valid loss : 1.02 - acc : 0.75 time taken: 11.48\n",
      "epoch 1 - batch [499/3968] - train loss: 0.95 - acc: 0.71 - valid loss : 0.97 - acc : 0.76 time taken: 12.22\n",
      "epoch 1 - batch [999/3968] - train loss: 0.46 - acc: 0.71 - valid loss : 1.00 - acc : 0.77 time taken: 12.24\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.31 - acc: 0.71 - valid loss : 0.99 - acc : 0.76 time taken: 12.24\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.23 - acc: 0.72 - valid loss : 0.98 - acc : 0.77 time taken: 12.26\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.18 - acc: 0.72 - valid loss : 1.01 - acc : 0.77 time taken: 12.24\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.15 - acc: 0.72 - valid loss : 0.98 - acc : 0.76 time taken: 12.25\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.13 - acc: 0.72 - valid loss : 0.99 - acc : 0.77 time taken: 12.27\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.11 - acc: 0.72 - valid loss : 0.96 - acc : 0.76 time taken: 11.52\n",
      "epoch 2 - batch [499/3968] - train loss: 0.84 - acc: 0.74 - valid loss : 0.96 - acc : 0.77 time taken: 12.26\n",
      "epoch 2 - batch [999/3968] - train loss: 0.42 - acc: 0.74 - valid loss : 0.95 - acc : 0.77 time taken: 12.23\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.28 - acc: 0.74 - valid loss : 0.97 - acc : 0.77 time taken: 12.27\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.21 - acc: 0.74 - valid loss : 0.98 - acc : 0.77 time taken: 12.25\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.17 - acc: 0.74 - valid loss : 0.97 - acc : 0.78 time taken: 12.26\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.14 - acc: 0.74 - valid loss : 0.96 - acc : 0.77 time taken: 12.27\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.12 - acc: 0.74 - valid loss : 0.95 - acc : 0.77 time taken: 12.26\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.10 - acc: 0.74 - valid loss : 0.95 - acc : 0.77 time taken: 11.47\n",
      "CPU times: user 4min 10s, sys: 41.2 s, total: 4min 51s\n",
      "Wall time: 4min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LSTM_clf(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (net): LSTM(300, 300, num_layers=2, dropout=0.5)\n",
       "  (relu): ReLU()\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(lstm1, 3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 1.21 - acc: 0.63 - valid loss : 1.05 - acc : 0.70 time taken: 10.93\n",
      "epoch 0 - batch [999/3968] - train loss: 0.56 - acc: 0.65 - valid loss : 1.03 - acc : 0.71 time taken: 11.09\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.36 - acc: 0.66 - valid loss : 1.01 - acc : 0.74 time taken: 10.87\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.25 - acc: 0.66 - valid loss : 1.00 - acc : 0.74 time taken: 10.99\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.20 - acc: 0.67 - valid loss : 0.91 - acc : 0.75 time taken: 10.98\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.16 - acc: 0.68 - valid loss : 0.90 - acc : 0.76 time taken: 10.99\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.13 - acc: 0.68 - valid loss : 0.92 - acc : 0.76 time taken: 11.03\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.10 - acc: 0.69 - valid loss : 0.93 - acc : 0.78 time taken: 10.31\n",
      "epoch 1 - batch [499/3968] - train loss: 0.61 - acc: 0.81 - valid loss : 0.90 - acc : 0.79 time taken: 11.05\n",
      "epoch 1 - batch [999/3968] - train loss: 0.29 - acc: 0.82 - valid loss : 0.91 - acc : 0.78 time taken: 11.03\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.19 - acc: 0.82 - valid loss : 0.92 - acc : 0.79 time taken: 11.08\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.14 - acc: 0.82 - valid loss : 0.92 - acc : 0.78 time taken: 11.09\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.11 - acc: 0.82 - valid loss : 0.92 - acc : 0.79 time taken: 11.04\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.09 - acc: 0.82 - valid loss : 0.91 - acc : 0.79 time taken: 11.06\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.08 - acc: 0.82 - valid loss : 0.95 - acc : 0.78 time taken: 11.07\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.06 - acc: 0.82 - valid loss : 0.95 - acc : 0.78 time taken: 10.37\n",
      "epoch 2 - batch [499/3968] - train loss: 0.36 - acc: 0.90 - valid loss : 0.94 - acc : 0.78 time taken: 11.07\n",
      "epoch 2 - batch [999/3968] - train loss: 0.18 - acc: 0.90 - valid loss : 0.94 - acc : 0.79 time taken: 11.10\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.12 - acc: 0.90 - valid loss : 0.96 - acc : 0.78 time taken: 11.07\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.09 - acc: 0.90 - valid loss : 0.96 - acc : 0.78 time taken: 11.09\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.07 - acc: 0.90 - valid loss : 0.99 - acc : 0.79 time taken: 11.04\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.06 - acc: 0.90 - valid loss : 0.96 - acc : 0.79 time taken: 11.13\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.05 - acc: 0.90 - valid loss : 0.97 - acc : 0.79 time taken: 11.07\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.04 - acc: 0.90 - valid loss : 0.99 - acc : 0.78 time taken: 10.38\n",
      "CPU times: user 3min 45s, sys: 37.2 s, total: 4min 22s\n",
      "Wall time: 4min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DDCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (dcnn): DCNN_rez_block(\n",
       "    (conv1): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (conv2): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 300, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (relu1): ReLU()\n",
       "    (relu2): ReLU()\n",
       "    (relu3): ReLU()\n",
       "    (do1): Dropout(p=0.2, inplace=False)\n",
       "    (do2): Dropout(p=0.2, inplace=False)\n",
       "    (do3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (cnn1): Conv1d(300, 100, kernel_size=(4,), stride=(1,), padding=(3,))\n",
       "  (cnn2): Conv1d(300, 100, kernel_size=(6,), stride=(1,), padding=(5,))\n",
       "  (cnn3): Conv1d(300, 100, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "  (bn): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=600, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(ddcnn_rez1, 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 - batch [499/3968] - train loss: 1.73 - acc: 0.48 - valid loss : 1.27 - acc : 0.64 time taken: 10.23\n",
      "epoch 0 - batch [999/3968] - train loss: 0.67 - acc: 0.53 - valid loss : 1.15 - acc : 0.66 time taken: 10.22\n",
      "epoch 0 - batch [1499/3968] - train loss: 0.40 - acc: 0.57 - valid loss : 1.11 - acc : 0.68 time taken: 10.25\n",
      "epoch 0 - batch [1999/3968] - train loss: 0.28 - acc: 0.59 - valid loss : 1.03 - acc : 0.71 time taken: 10.26\n",
      "epoch 0 - batch [2499/3968] - train loss: 0.21 - acc: 0.61 - valid loss : 0.96 - acc : 0.73 time taken: 10.29\n",
      "epoch 0 - batch [2999/3968] - train loss: 0.17 - acc: 0.62 - valid loss : 0.92 - acc : 0.74 time taken: 10.77\n",
      "epoch 0 - batch [3499/3968] - train loss: 0.14 - acc: 0.63 - valid loss : 0.91 - acc : 0.77 time taken: 11.88\n",
      "epoch 0 - batch [3967/3968] - train loss: 0.11 - acc: 0.64 - valid loss : 0.91 - acc : 0.76 time taken: 10.72\n",
      "epoch 1 - batch [499/3968] - train loss: 0.72 - acc: 0.78 - valid loss : 0.92 - acc : 0.77 time taken: 10.34\n",
      "epoch 1 - batch [999/3968] - train loss: 0.35 - acc: 0.78 - valid loss : 0.91 - acc : 0.77 time taken: 10.30\n",
      "epoch 1 - batch [1499/3968] - train loss: 0.23 - acc: 0.79 - valid loss : 0.92 - acc : 0.78 time taken: 10.33\n",
      "epoch 1 - batch [1999/3968] - train loss: 0.17 - acc: 0.79 - valid loss : 0.91 - acc : 0.77 time taken: 10.56\n",
      "epoch 1 - batch [2499/3968] - train loss: 0.13 - acc: 0.79 - valid loss : 0.86 - acc : 0.77 time taken: 10.45\n",
      "epoch 1 - batch [2999/3968] - train loss: 0.11 - acc: 0.79 - valid loss : 0.84 - acc : 0.77 time taken: 10.33\n",
      "epoch 1 - batch [3499/3968] - train loss: 0.09 - acc: 0.79 - valid loss : 0.88 - acc : 0.78 time taken: 10.31\n",
      "epoch 1 - batch [3967/3968] - train loss: 0.08 - acc: 0.79 - valid loss : 0.85 - acc : 0.79 time taken: 9.67\n",
      "epoch 2 - batch [499/3968] - train loss: 0.50 - acc: 0.85 - valid loss : 0.86 - acc : 0.79 time taken: 10.27\n",
      "epoch 2 - batch [999/3968] - train loss: 0.25 - acc: 0.85 - valid loss : 0.88 - acc : 0.78 time taken: 10.30\n",
      "epoch 2 - batch [1499/3968] - train loss: 0.16 - acc: 0.85 - valid loss : 0.89 - acc : 0.78 time taken: 10.33\n",
      "epoch 2 - batch [1999/3968] - train loss: 0.12 - acc: 0.85 - valid loss : 0.89 - acc : 0.78 time taken: 10.29\n",
      "epoch 2 - batch [2499/3968] - train loss: 0.10 - acc: 0.85 - valid loss : 0.89 - acc : 0.78 time taken: 10.34\n",
      "epoch 2 - batch [2999/3968] - train loss: 0.08 - acc: 0.85 - valid loss : 0.89 - acc : 0.78 time taken: 11.12\n",
      "epoch 2 - batch [3499/3968] - train loss: 0.07 - acc: 0.85 - valid loss : 0.89 - acc : 0.78 time taken: 11.76\n",
      "epoch 2 - batch [3967/3968] - train loss: 0.06 - acc: 0.85 - valid loss : 0.90 - acc : 0.78 time taken: 9.97\n",
      "CPU times: user 3min 37s, sys: 33.3 s, total: 4min 10s\n",
      "Wall time: 4min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DDCNN(\n",
       "  (word_embedding): Embedding(113137, 300)\n",
       "  (dcnn): DCNN_rez_block(\n",
       "    (conv1): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(2,))\n",
       "    (conv2): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(4,), dilation=(2,))\n",
       "    (conv3): Conv1d(300, 150, kernel_size=(3,), stride=(1,), padding=(8,), dilation=(4,))\n",
       "    (relu1): ReLU()\n",
       "    (relu2): ReLU()\n",
       "    (relu3): ReLU()\n",
       "    (do1): Dropout(p=0.2, inplace=False)\n",
       "    (do2): Dropout(p=0.2, inplace=False)\n",
       "    (do3): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (cnn1): Conv1d(300, 50, kernel_size=(4,), stride=(1,), padding=(3,))\n",
       "  (cnn2): Conv1d(300, 50, kernel_size=(6,), stride=(1,), padding=(5,))\n",
       "  (cnn3): Conv1d(300, 50, kernel_size=(8,), stride=(1,), padding=(7,))\n",
       "  (bn): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=300, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time training(ddcnn_rez2, 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_6OH2OCfLDV"
   },
   "outputs": [],
   "source": [
    "models = [dcnn1, dcnn2, dcnn_rez1, dcnn_rez2, lstm1, ddcnn_rez1, ddcnn_rez2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "-CHlD2gdZyin",
    "outputId": "ce50190f-26b1-4ab5-ba41-36949747f7aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7838258164852255\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.7993779160186625\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.7978227060653188\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.7884914463452566\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.7744945567651633\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.7807153965785381\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n",
      "accuracy:  0.776049766718507\n",
      "verify predicts won't mess up order 1.0\n",
      "generating test preds\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "valid_preds_dict = {}\n",
    "test_preds_dict = {}\n",
    "for i, model in enumerate(models):\n",
    "    _, valid_preds, valid_labels, valid_loss = validate(model, criterion)\n",
    "    print(\"accuracy: \", accuracy(valid_preds, valid_labels))\n",
    "    valid_preds_ = predict(model, valid_loader)\n",
    "    print(\"verify predicts won't mess up order\", accuracy(valid_preds, valid_preds))\n",
    "    print(\"generating test preds\")\n",
    "    test_preds = predict(model, test_loader)\n",
    "    \n",
    "    valid_preds_dict[\"model_{}\".format(i)] = valid_preds_\n",
    "    test_preds_dict[\"model_{}\".format(i)] = test_preds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds = pd.DataFrame(valid_preds_dict)\n",
    "test_preds = pd.DataFrame(test_preds_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_preds[\"majority_vote\"] = valid_preds.apply(lambda row: row.value_counts().index[0].astype(int), axis=1)\n",
    "test_preds[\"majority_vote\"] = test_preds.apply(lambda row: row.value_counts().index[0].astype(int), axis=1)\n",
    "valid_preds[\"pred_label\"] = valid_preds[\"majority_vote\"].apply(lambda x: label_vocab.i2w[x])\n",
    "test_preds[\"pred_label\"] = test_preds[\"majority_vote\"].apply(lambda x: label_vocab.i2w[x])\n",
    "valid_preds.to_csv(\"valid_predictions.csv\", index=False)\n",
    "test_preds.to_csv(\"test_predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_0</th>\n",
       "      <th>model_1</th>\n",
       "      <th>model_2</th>\n",
       "      <th>model_3</th>\n",
       "      <th>model_4</th>\n",
       "      <th>model_5</th>\n",
       "      <th>model_6</th>\n",
       "      <th>majority_vote</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>sports and recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>sports and recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>sports and recreation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>media and drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_0  model_1  model_2  model_3  model_4  model_5  model_6  \\\n",
       "0       13       13       13       13       13       13       13   \n",
       "1       13       13       13       13       13       13       13   \n",
       "2       13       13       13       13       13       13       13   \n",
       "3        7        7        7        7        7        7        7   \n",
       "4        9        4        7        9        9        4       11   \n",
       "\n",
       "   majority_vote             pred_label  \n",
       "0             13  sports and recreation  \n",
       "1             13  sports and recreation  \n",
       "2             13  sports and recreation  \n",
       "3              7        media and drama  \n",
       "4              9                  music  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MA0-uWKIVkVG"
   },
   "source": [
    "#### Prediction\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-4DtMu9MCp7r",
    "outputId": "0c1245eb-a46d-41aa-a118-b8f8382edaa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8227060653188181\n"
     ]
    }
   ],
   "source": [
    "print(accuracy(valid_preds[\"majority_vote\"].values, valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vP3cFgX2Cq0R"
   },
   "outputs": [],
   "source": [
    "with open(\"dev_results.txt\", \"w\") as f:\n",
    "    for s in label_vocab.map_index2words(valid_preds.majority_vote.values):\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H3XvHomXE63x"
   },
   "outputs": [],
   "source": [
    "with open(\"test_results.txt\", \"w\") as f:\n",
    "    for s in label_vocab.map_index2words(test_preds.majority_vote.values):\n",
    "        f.write(s + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kerzFQVNFWY_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw1-747-keras.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
