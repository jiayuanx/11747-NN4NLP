{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd-iBU6A4wAg"
   },
   "source": [
    "### Assignment 1 Text Classification\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "iBO9z3rZ4wAk",
    "outputId": "9e292757-d3dc-4120-eb3c-cf9dfa547470"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-02-02 18:26:20--  http://phontron.com/data/topicclass-v1.tar.gz\n",
      "Resolving phontron.com (phontron.com)... 208.113.196.149\n",
      "Connecting to phontron.com (phontron.com)|208.113.196.149|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15665160 (15M) [application/gzip]\n",
      "Saving to: ‘topicclass-v1.tar.gz’\n",
      "\n",
      "topicclass-v1.tar.g 100%[===================>]  14.94M  43.6MB/s    in 0.3s    \n",
      "\n",
      "2020-02-02 18:26:21 (43.6 MB/s) - ‘topicclass-v1.tar.gz’ saved [15665160/15665160]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://phontron.com/data/topicclass-v1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ZsPjiIIl4wAq",
    "outputId": "77a791d8-63c0-42c4-a7a1-8feebb838217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topicclass/\n",
      "topicclass/topicclass_valid.txt\n",
      "topicclass/topicclass_test.txt\n",
      "topicclass/topicclass_train.txt\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf topicclass-v1.tar.gz topicclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "kZVUSq0u4-DJ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "58f96062-e225-4e70-cafd-798960887d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting allennlp\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6MB 3.4MB/s \n",
      "\u001b[?25hCollecting flaky\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n",
      "Collecting parsimonious>=0.8.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.0MB/s \n",
      "\u001b[?25hCollecting flask-cors>=3.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting word2number>=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Collecting pytorch-transformers==1.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 57.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.1)\n",
      "Collecting overrides\n",
      "  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n",
      "Collecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fe/a6/e69e38f1f259fcf8532d8bd2c4bc88764f42d7b35a41423a7f4b035cc5ce/jsonnet-0.14.0.tar.gz (253kB)\n",
      "\u001b[K     |████████████████████████████████| 256kB 53.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.17.5)\n",
      "Requirement already satisfied: gevent>=1.3.6 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.10.47)\n",
      "Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.3.1)\n",
      "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n",
      "Collecting conllu==1.3.1\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n",
      "Collecting unidecode\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001b[K     |████████████████████████████████| 245kB 55.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n",
      "Collecting ftfy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ca/2d9a5030eaf1bcd925dab392762b9709a7ad4bd486a90599d93cd79cb188/ftfy-5.6.tar.gz (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 9.6MB/s \n",
      "\u001b[?25hCollecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.1)\n",
      "Collecting pytorch-pretrained-bert>=0.6.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 54.2MB/s \n",
      "\u001b[?25hCollecting tensorboardX>=1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001b[K     |████████████████████████████████| 204kB 57.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.1.2)\n",
      "Collecting responses>=0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/0c/940781dd49710f4b1f0650c450c9fd8491db0e1bffd99ebc36355607f96d/responses-0.10.9-py2.py3-none-any.whl\n",
      "Collecting numpydoc>=0.8.0\n",
      "  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n",
      "Requirement already satisfied: spacy<2.2,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.1.9)\n",
      "Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n",
      "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.28.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from parsimonious>=0.8.0->allennlp) (1.12.0)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 30.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers==1.1.0->allennlp) (2019.12.20)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n",
      "Requirement already satisfied: greenlet>=0.4.14; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.6/dist-packages (from gevent>=1.3.6->allennlp) (0.4.15)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.13.47)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (42.0.2)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.0.2)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.8)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n",
      "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (2.10.3)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (0.16.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.6.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n",
      "Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.9.6)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.1)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.1)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.2.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (7.0.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->allennlp) (0.15.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask>=1.0.2->allennlp) (1.1.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.0)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.1.2)\n",
      "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n",
      "Building wheels for collected packages: parsimonious, word2number, overrides, jsonnet, ftfy, numpydoc\n",
      "  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42709 sha256=ee0860480624ab35cfa987d473e0e26ee4648825f800559bd655fc9acd9783be\n",
      "  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n",
      "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5588 sha256=9493399de679c4132f85c9cc646ae4f7399a80b74c8e23f3a787dba0baa1376a\n",
      "  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n",
      "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5608 sha256=e5de82562e2935902598864de93d14befda3091019af8ccb7aebf726da755b1f\n",
      "  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n",
      "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jsonnet: filename=jsonnet-0.14.0-cp36-cp36m-linux_x86_64.whl size=3320359 sha256=17ddc40d4eced3ef9e38ce26da89654c339c5ef31f64e03d64725a044108c6b3\n",
      "  Stored in directory: /root/.cache/pip/wheels/5b/b7/83/985f0f758fbb34f14989a0fab86d18890d1cc5ae12f26967bc\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-5.6-cp36-none-any.whl size=44553 sha256=de71760613f624b9796ccb803f04ce3a3f8d93405c2e37b6e52ea60b7125b91c\n",
      "  Stored in directory: /root/.cache/pip/wheels/43/34/ce/cbb38d71543c408de56f3c5e26ce8ba495a0fa5a28eaaf1046\n",
      "  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31894 sha256=72ac7301dfa54542a14d042e08b3e89e6d80572dcb2611542a247cc492432d3f\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n",
      "Successfully built parsimonious word2number overrides jsonnet ftfy numpydoc\n",
      "Installing collected packages: flaky, parsimonious, flask-cors, word2number, sentencepiece, pytorch-transformers, overrides, jsonnet, conllu, unidecode, ftfy, jsonpickle, pytorch-pretrained-bert, tensorboardX, responses, numpydoc, allennlp\n",
      "Successfully installed allennlp-0.9.0 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.6 jsonnet-0.14.0 jsonpickle-1.2 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.9 sentencepiece-0.1.85 tensorboardX-2.0 unidecode-1.1.1 word2number-1.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install allennlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vt4GDb8B4wAu"
   },
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor, TextClassifierPredictor\n",
    "from allennlp.training.checkpointer import Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kISwWXT4wAy"
   },
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \n",
    "    def __init__(self, token_indexers=None):\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        \n",
    "    def text_to_instance(self, tokens, label=None):\n",
    "        if isinstance(tokens[0], str):\n",
    "            tokens = map(Token, tokens)\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "        \n",
    "        if label:\n",
    "            label_field = LabelField(label=label)\n",
    "            fields[\"label\"] = label_field\n",
    "            \n",
    "        return Instance(fields)\n",
    "    \n",
    "    def _read(self, file_path):\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                label, sent = line.strip().split(\"|||\")\n",
    "                sent, label = sent.strip(), label.strip()\n",
    "                if label == \"UNK\":\n",
    "                    label = None\n",
    "                yield self.text_to_instance([Token(word) for word in sent.split(\" \")], label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V5B9GKOD4wA2"
   },
   "outputs": [],
   "source": [
    "class LstmTagger(Model):\n",
    "    def __init__(self, word_embeddings, encoder, vocab):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.maxpool = torch.nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                   out_features=vocab.get_vocab_size(\"labels\"))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, sentence, label=None):\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        encoder_out = encoder_out.transpose(1,2)\n",
    "        out = self.maxpool(encoder_out).transpose(1,2).squeeze() # max pool across time\n",
    "        out = self.out(out)\n",
    "        output = {\"logits\": out}\n",
    "\n",
    "        if label is not None:\n",
    "            self.accuracy(out, label)\n",
    "            output[\"loss\"] = self.criterion(out, label)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset):\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJ2bsiMy4wA4"
   },
   "outputs": [],
   "source": [
    "reader = PosDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "SrLkGxT14wA9",
    "outputId": "cec6ae96-b0e9-4b15-f79d-42f1fe1d28c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "253909it [00:20, 12606.28it/s]\n",
      "643it [00:00, 14982.07it/s]\n",
      "697it [00:00, 13994.26it/s]\n"
     ]
    }
   ],
   "source": [
    "TRAIN = \"topicclass/topicclass_train.txt\"\n",
    "VALID = \"topicclass/topicclass_valid.txt\"\n",
    "TEST = \"topicclass/topicclass_test.txt\"\n",
    "\n",
    "train_dataset = reader.read(cached_path(TRAIN))\n",
    "valid_dataset = reader.read(cached_path(VALID))\n",
    "test_dataset = reader.read(cached_path(TEST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "19O25ZwE4wBA",
    "outputId": "dcffae1b-0555-4990-cb5a-52323d6cce1c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': {'sentence': <allennlp.data.fields.text_field.TextField at 0x1a89b8b0f0>},\n",
       " 'indexed': False}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "reoltZ7Z4wBH",
    "outputId": "895c974d-03e1-49cf-b986-b4ce29541d6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [The,\n",
       "  Māori,\n",
       "  players,\n",
       "  initially,\n",
       "  provoked,\n",
       "  curiosity,\n",
       "  due,\n",
       "  to,\n",
       "  their,\n",
       "  race,\n",
       "  ,,\n",
       "  but,\n",
       "  the,\n",
       "  British,\n",
       "  press,\n",
       "  subsequently,\n",
       "  expressed,\n",
       "  some,\n",
       "  surprise,\n",
       "  that,\n",
       "  the,\n",
       "  side,\n",
       "  was,\n",
       "  not,\n",
       "  as,\n",
       "  \",\n",
       "  Māori,\n",
       "  \",\n",
       "  as,\n",
       "  they,\n",
       "  had,\n",
       "  expected,\n",
       "  .],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x1a326bb8d0>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None,\n",
       " '_token_index_to_indexer_name': None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset[0].fields[\"sentence\"].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kQ4H5z7Z4wBK",
    "outputId": "f67c5480-a7ee-47c5-904a-84212f691985"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254552/254552 [00:07<00:00, 32282.07it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ZHgZ9Avm4wBP",
    "outputId": "91a7f7f4-eedf-489b-b753-dfe7968e6580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: the\t\tFrequency: 379873\n",
      "\tToken: ,\t\tFrequency: 329376\n",
      "\tToken: .\t\tFrequency: 254552\n",
      "\tToken: of\t\tFrequency: 195666\n",
      "\tToken: and\t\tFrequency: 188073\n",
      "\tToken: in\t\tFrequency: 156502\n",
      "\tToken: a\t\tFrequency: 115405\n",
      "\tToken: to\t\tFrequency: 105341\n",
      "\tToken: was\t\tFrequency: 84184\n",
      "\tToken: The\t\tFrequency: 67019\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: 71828182845904523536028747135266249775724709369995\t\tlength: 50\tFrequency: 1\n",
      "\tToken: GlennBeckRapedAndMurderedAYoungGirlIn1990.com\t\tlength: 45\tFrequency: 1\n",
      "\tToken: Andriantsimitoviaminandriandrazaka\t\tlength: 34\tFrequency: 1\n",
      "\tToken: Southernplayalisticadillacmuzik\t\tlength: 31\tFrequency: 4\n",
      "\tToken: Andriantsimitoviaminiandriana\t\tlength: 29\tFrequency: 5\n",
      "\tToken: Kollektivtransportproduksjon\t\tlength: 28\tFrequency: 5\n",
      "\tToken: 113423713055421844361000443\t\tlength: 27\tFrequency: 1\n",
      "\tToken: Landesversicherungsanstalt\t\tlength: 26\tFrequency: 2\n",
      "\tToken: Rabodoandrianampoinimerina\t\tlength: 26\tFrequency: 1\n",
      "\tToken: chlorobenzalmalononitrile\t\tlength: 25\tFrequency: 2\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: \t\tlength: 0\tFrequency: 10\n",
      "\tToken: ﻿\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ρ\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ･\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ®\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ʔ\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ̆\t\tlength: 1\tFrequency: 1\n",
      "\tToken: ア\t\tlength: 1\tFrequency: 1\n",
      "\tToken: द\t\tlength: 1\tFrequency: 1\n",
      "\tToken: و\t\tlength: 1\tFrequency: 1\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'labels':\n",
      "\tToken: Music\t\tFrequency: 33828\n",
      "\tToken: Sports and recreation\t\tFrequency: 31334\n",
      "\tToken: Natural sciences\t\tFrequency: 29749\n",
      "\tToken: Warfare\t\tFrequency: 26217\n",
      "\tToken: Media and drama\t\tFrequency: 25506\n",
      "\tToken: Social sciences and society\t\tFrequency: 20518\n",
      "\tToken: History\t\tFrequency: 18470\n",
      "\tToken: Engineering and technology\t\tFrequency: 18407\n",
      "\tToken: Geography and places\t\tFrequency: 12302\n",
      "\tToken: Video games\t\tFrequency: 11863\n",
      "\n",
      "Top 10 longest tokens in namespace 'labels':\n",
      "\tToken: Social sciences and society\t\tlength: 27\tFrequency: 20518\n",
      "\tToken: Agriculture, food and drink\t\tlength: 27\tFrequency: 1759\n",
      "\tToken: Engineering and technology\t\tlength: 26\tFrequency: 18407\n",
      "\tToken: Language and literature\t\tlength: 23\tFrequency: 7448\n",
      "\tToken: Philosophy and religion\t\tlength: 23\tFrequency: 5309\n",
      "\tToken: Sports and recreation\t\tlength: 21\tFrequency: 31334\n",
      "\tToken: Geography and places\t\tlength: 20\tFrequency: 12302\n",
      "\tToken: Art and architecture\t\tlength: 20\tFrequency: 10238\n",
      "\tToken: Natural sciences\t\tlength: 16\tFrequency: 29749\n",
      "\tToken: Media and darama\t\tlength: 16\tFrequency: 11\n",
      "\n",
      "Top 10 shortest tokens in namespace 'labels':\n",
      "\tToken: Music\t\tlength: 5\tFrequency: 33828\n",
      "\tToken: History\t\tlength: 7\tFrequency: 18470\n",
      "\tToken: Warfare\t\tlength: 7\tFrequency: 26217\n",
      "\tToken: Mathematics\t\tlength: 11\tFrequency: 679\n",
      "\tToken: Video games\t\tlength: 11\tFrequency: 11863\n",
      "\tToken: Miscellaneous\t\tlength: 13\tFrequency: 914\n",
      "\tToken: Media and drama\t\tlength: 15\tFrequency: 25506\n",
      "\tToken: Media and darama\t\tlength: 16\tFrequency: 11\n",
      "\tToken: Natural sciences\t\tlength: 16\tFrequency: 29749\n",
      "\tToken: Art and architecture\t\tlength: 20\tFrequency: 10238\n"
     ]
    }
   ],
   "source": [
    "vocab.print_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eQSAqBei4wBS"
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"tokens\"),\n",
    "                            embedding_dim=EMBED_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evbjOO1N4wBW"
   },
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBED_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = LstmTagger(word_embeddings, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "toruosxm4wBY"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "iterator = BucketIterator(batch_size=64, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "checkpointer = Checkpointer(\"./checkpoints/ckpt-1\", 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "W6Yqg3SO4wBc",
    "outputId": "e5d0165c-33e1-406b-a6c8-28f8508a988e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 0,\n",
       " 'best_validation_accuracy': 0.8040435458786936,\n",
       " 'best_validation_loss': 0.92332116717642}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=valid_dataset,\n",
    "                  patience=5,\n",
    "                  num_epochs=3,\n",
    "                  checkpointer=checkpointer,\n",
    "                  cuda_device=cuda_device)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict\n",
    "\n",
    "from allennlp.data.iterators import DataIterator\n",
    "from tqdm import tqdm\n",
    "from scipy.special import expit # the sigmoid function\n",
    " \n",
    "def tonp(tsr): return tsr.detach().cpu().numpy()\n",
    " \n",
    "class Predictor:\n",
    "    def __init__(self, model: Model, iterator: DataIterator,\n",
    "                 cuda_device: int=-1) -> None:\n",
    "        self.model = model\n",
    "        self.iterator = iterator\n",
    "        self.cuda_device = cuda_device\n",
    "         \n",
    "    def _extract_data(self, batch) -> np.ndarray:\n",
    "        out_dict = self.model(**batch)\n",
    "        return expit(tonp(out_dict[\"logits\"]))\n",
    "     \n",
    "    def predict(self, ds) -> np.ndarray:\n",
    "        pred_generator = self.iterator(ds, num_epochs=1, shuffle=False)\n",
    "        self.model.eval()\n",
    "        pred_generator_tqdm = tqdm(pred_generator,\n",
    "                                   total=self.iterator.get_num_batches(ds))\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch in pred_generator_tqdm:\n",
    "                batch = nn_util.move_to_device(batch, self.cuda_device)\n",
    "                preds.append(self._extract_data(batch))\n",
    "        return np.concatenate(preds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.nn import util as nn_util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TextClassifierPredictor(model, reader)\n",
    "logits = predictor.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7DbxYkAS62w1"
   },
   "source": [
    "#### TCN\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4_kBlTgV64qI"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size):\n",
    "        super(Chomp1d, self).__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "  \n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp1 = Chomp1d(padding)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n",
    "                                           stride=stride, padding=padding, dilation=dilation))\n",
    "        self.chomp2 = Chomp1d(padding)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2yPF4J_v66sN"
   },
   "outputs": [],
   "source": [
    "class TCNClassifier(Model):\n",
    "    def __init__(self, word_embeddings, vocab, dropout=0.5):\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = TemporalConvNet(EMBED_DIM, [HIDDEN_DIM] * N_LAYERS, \n",
    "                                       kernel_size=KERNEL_SIZE, \n",
    "                                       dropout=dropout)\n",
    "        self.maxpool = torch.nn.AdaptiveMaxPool1d(1)\n",
    "        self.out = torch.nn.Linear(in_features=HIDDEN_DIM,\n",
    "                                   out_features=vocab.get_vocab_size(\"labels\"))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, sentence, label):\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings.transpose(1,2))\n",
    "        out = self.maxpool(encoder_out).transpose(1,2).squeeze() # max pool across time\n",
    "        out = self.out(out)\n",
    "        output = {\"logits\": out}\n",
    "\n",
    "        if label is not None:\n",
    "            self.accuracy(out, label)\n",
    "            output[\"loss\"] = self.criterion(out, label)\n",
    "        return output\n",
    "    \n",
    "    def get_metrics(self, reset):\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WMOnHNsDVua"
   },
   "outputs": [],
   "source": [
    "from allennlp.training.learning_rate_schedulers import learning_rate_scheduler\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from allennlp.training.checkpointer import Checkpointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "n6CZHWU--wh0",
    "outputId": "3fd79c1f-8c78-4648-e029-b651aec0f6a9"
   },
   "outputs": [],
   "source": [
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 300\n",
    "N_LAYERS = 3\n",
    "KERNEL_SIZE = 5\n",
    "WD = 0\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"tokens\"),\n",
    "                            embedding_dim=EMBED_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "model = TCNClassifier(word_embeddings, vocab)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=WD)\n",
    "iterator = BucketIterator(batch_size=64, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "checkpointer = Checkpointer(\"./checkpoints/ckpt-2\", 30)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=valid_dataset,\n",
    "                  patience=5,\n",
    "                  num_epochs=3,\n",
    "                  grad_clipping=1.0,\n",
    "                  cuda_device=cuda_device,\n",
    "                  checkpointer=checkpointer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bc_0uBo2Pc9z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "accuracy: 0.4711, loss: 1.5759 ||: 100%|██████████| 3968/3968 [01:33<00:00, 42.31it/s]\n",
      "accuracy: 0.6719, loss: 1.2086 ||: 100%|██████████| 11/11 [00:00<00:00, 158.74it/s]\n",
      "accuracy: 0.7106, loss: 0.9358 ||: 100%|██████████| 3968/3968 [01:26<00:00, 45.79it/s]\n",
      "accuracy: 0.7589, loss: 1.0635 ||: 100%|██████████| 11/11 [00:00<00:00, 210.23it/s]\n",
      "accuracy: 0.7921, loss: 0.6982 ||: 100%|██████████| 3968/3968 [01:26<00:00, 45.71it/s]\n",
      "accuracy: 0.7621, loss: 1.0758 ||: 100%|██████████| 11/11 [00:00<00:00, 171.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 1,\n",
       " 'peak_cpu_memory_MB': 5321.216,\n",
       " 'peak_gpu_0_memory_MB': 2474,\n",
       " 'training_duration': '0:04:28.852414',\n",
       " 'training_start_epoch': 0,\n",
       " 'training_epochs': 2,\n",
       " 'epoch': 2,\n",
       " 'training_accuracy': 0.7920514830116302,\n",
       " 'training_loss': 0.698194510836695,\n",
       " 'training_cpu_memory_MB': 5321.216,\n",
       " 'training_gpu_0_memory_MB': 2474,\n",
       " 'validation_accuracy': 0.7620528771384136,\n",
       " 'validation_loss': 1.0757639489390634,\n",
       " 'best_validation_accuracy': 0.7589424572317263,\n",
       " 'best_validation_loss': 1.0634593584320762}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBED_DIM = 300\n",
    "HIDDEN_DIM = 100\n",
    "N_LAYERS = 5\n",
    "KERNEL_SIZE = 3\n",
    "WD = 0\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size(\"tokens\"),\n",
    "                            embedding_dim=EMBED_DIM)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})\n",
    "\n",
    "model = TCNClassifier(word_embeddings, vocab)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_device = 0\n",
    "    model = model.cuda(cuda_device)\n",
    "else:\n",
    "    cuda_device = -1\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=WD)\n",
    "iterator = BucketIterator(batch_size=64, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "checkpointer = Checkpointer(\"./checkpoints/ckpt-3\", 30)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=valid_dataset,\n",
    "                  patience=5,\n",
    "                  num_epochs=3,\n",
    "                  grad_clipping=1.0,\n",
    "                  cuda_device=cuda_device,\n",
    "                  checkpointer=checkpointer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and evaluate\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors import TextClassifierPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = LstmTagger(word_embeddings, lstm, vocab)\n",
    "with open(\"checkpoints/ckpt-1/best.th\", \"rb\") as f:\n",
    "    model1.load_state_dict(torch.load(f))\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_device = 0\n",
    "        model1 = model1.cuda(cuda_device)\n",
    "    else:\n",
    "        cuda_device = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor1 = TextClassifierPredictor(model1, dataset_reader=reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "11747-HW1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
